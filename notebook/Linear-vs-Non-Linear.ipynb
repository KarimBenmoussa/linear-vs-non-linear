{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear vs. Non-linear classifiers: An extensive benchmark on OpenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine some examplar setup ids\n",
    "\n",
    "examplar_setup_ids_svm = (7130285, 7130286)\n",
    "examplar_setup_ids_ann = (7130157, 7130159)\n",
    "examplar_setup_ids_dt  = (7130853, 7130854)\n",
    "\n",
    "examplar_setup_ids_all = {'Support Vector Machines': examplar_setup_ids_svm, \n",
    "                          'Neural Networks': examplar_setup_ids_ann, \n",
    "                          'Decision Trees': examplar_setup_ids_dt}\n",
    "\n",
    "study_id = 123\n",
    "tag = 'study_%d' % study_id\n",
    "study = openml.study.get_study(study_id, 'tasks')\n",
    "measure = 'predictive_accuracy'\n",
    "\n",
    "meta_feature_x_axis = 'NumberOfInstances'\n",
    "meta_feature_y_axis = 'NumberOfFeatures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the meta-features per task. \n",
    "# Note: this will download (and cache) all involved datasets; might take a while\n",
    "# The output of this cell is only required for the scatterplots\n",
    "\n",
    "task_metafeatures = dict()\n",
    "for task_id in study.tasks:\n",
    "    current_task = openml.tasks.get_task(task_id)\n",
    "    task_metafeatures[task_id] = current_task.get_dataset().qualities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the evaluations\n",
    "setup_task_evaluation = collections.defaultdict(dict)\n",
    "for classifier_family, examplars in examplar_setup_ids_all.items():\n",
    "    for setup_id in examplars:\n",
    "        examplar_setup = openml.setups.get_setup(setup_id)\n",
    "        all_setups = openml.setups.list_partial_setups(examplar_setup, ['categorical_features', 'random_state'])\n",
    "        evaluations = openml.evaluations.list_evaluations(measure,\n",
    "                                                          flow=[examplar_setup.flow_id], \n",
    "                                                          setup=all_setups.keys(), \n",
    "                                                          task=study.tasks)\n",
    "        for run_id, evaluation in evaluations.items():\n",
    "            setup_task_evaluation[setup_id][evaluation.task_id] = evaluation.value\n",
    "        print('[%s] Obtained %d evaluations for examplar setup_id %d' % (classifier_family, \n",
    "                                                                       len(setup_task_evaluation[setup_id]),\n",
    "                                                                       setup_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference plots\n",
    "Produces the differences plots as in the paper. These give an idea how often a given type (linear / non-linear) is better and with how much. We expect the non-linear classifier to be better in most cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S-Plot:\n",
    "for classifier_family, examplars in examplar_setup_ids_all.items():\n",
    "    id_linear = examplars[0]\n",
    "    id_nonlinear = examplars[1]\n",
    "    differences = []\n",
    "    for task_id in setup_task_evaluation[id_linear].keys():\n",
    "        if task_id in setup_task_evaluation[id_nonlinear].keys():\n",
    "            score_linear = setup_task_evaluation[id_linear][task_id]\n",
    "            score_nonlinear = setup_task_evaluation[id_nonlinear][task_id]\n",
    "            differences.append(score_linear - score_nonlinear)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(differences)), sorted(differences))\n",
    "    ax.set_title(classifier_family)\n",
    "    ax.set_xlabel('Dataset (sorted)')\n",
    "    ax.set_ylabel('perf non-linear - perf linear')\n",
    "    ax.grid(linestyle='--', axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plots \n",
    "This script produces scatterplots similar to the ones presented in the paper. Note that there is a big difference: The scatterplots in the paper are produced based on a statistical test, the scatterplots in this notebook are based on absolute difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplots\n",
    "colors = ['blue', 'gray', 'red']\n",
    "labels = ['linear', 'equal', 'non-linear']\n",
    "\n",
    "for classifier_family, examplars in examplar_setup_ids_all.items():\n",
    "    id_linear = examplars[0]\n",
    "    id_nonlinear = examplars[1]\n",
    "    all_results = collections.defaultdict(list)\n",
    "    \n",
    "    for task_id in setup_task_evaluation[id_linear].keys():\n",
    "        if task_id in setup_task_evaluation[id_nonlinear].keys():\n",
    "            task_coords = [task_metafeatures[task_id][meta_feature_x_axis], \n",
    "                           task_metafeatures[task_id][meta_feature_y_axis]]\n",
    "            score_linear = setup_task_evaluation[id_linear][task_id]\n",
    "            score_nonlinear = setup_task_evaluation[id_nonlinear][task_id]\n",
    "            \n",
    "            # fill the all_results dict using the keys in labels\n",
    "            if score_linear > score_nonlinear:\n",
    "                all_results['linear'].append(task_coords)\n",
    "            elif score_nonlinear > score_linear:\n",
    "                all_results['non-linear'].append(task_coords)\n",
    "            else:\n",
    "                all_results['equal'].append(task_coords)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for idx, result in enumerate(labels):\n",
    "        # make results numpy arrays, for slicing\n",
    "        all_results[result] = np.array(all_results[result])\n",
    "        plt.scatter(all_results[result][:, 0], all_results[result][:, 1], c=colors[idx], alpha=0.5, label=labels[idx])\n",
    "    ax.set_title(classifier_family)\n",
    "    ax.set_xlabel(meta_feature_x_axis)\n",
    "    ax.set_ylabel(meta_feature_y_axis)\n",
    "    ax.legend()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
